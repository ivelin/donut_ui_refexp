{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivelin/donut_ui_refexp/blob/main/Trainer_Hyper_Parameter_Tuning_for_Donut_UI_RefExp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNMqJ821yNVo"
      },
      "source": [
        "# Fine-tune Donut ğŸ© on UI RefExp\n",
        "\n",
        "This Notebook supplements the Fine Tuning notebook. It uses the Pytorch Lightning tuner to find optimal hyper-parameters for best utilization of underlying training hardware.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot1nP9YHz8co",
        "outputId": "2e8e56fe-c977-4310-db6f-bf82da056e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OqcGNPJHyOlt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "637cf8f7-3018-4918-f38f-e2f625fa9c2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m452.9/452.9 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMZ6tiMB1JxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fddd8432-616c-4bac-f935-b8f56b9cbcd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m825.8/825.8 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m178.9/178.9 KB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pytorch-lightning wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Login to Weights&Biases so we can log and chart our training metrics\n",
        "!wandb login"
      ],
      "metadata": {
        "id": "GWoh-cGbnWpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWYic8VNyDNU"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "Next, let's load the dataset from the [hub](https://huggingface.co/datasets/naver-clova-ix/cord-v2). We're prepared a minimal dataset for DocVQA, the notebook for that can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Donut/DocVQA/Creating_a_toy_DocVQA_dataset_for_Donut.ipynb).\n",
        "\n",
        "Important here is that we've added a \"ground_truth\" column, containing the ground truth JSON which the model will learn to generate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hU27XC2yEot"
      },
      "outputs": [],
      "source": [
        "REFEXP_DATASET_NAME = \"ivelin/rico_refexp_combined\"\n",
        "\n",
        "# Pick which pretrained checkpoint to start the fine tuning process from\n",
        "REFEXP_MODEL_CHECKPOINT = \"ivelin/donut-refexp-combined-v1\"\n",
        "\n",
        "# REFEXP_MODEL_CHECKPOINT = \"ivelin/donut-refexp-draft\"\n",
        "# REFEXP_MODEL_CHECKPOINT = \"naver-clova-ix/donut-base\"\n",
        "# REFEXP_MODEL_CHECKPOINT = \"ivelin/donut-docvqa-demo\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(REFEXP_DATASET_NAME, num_proc=8)"
      ],
      "metadata": {
        "id": "hSguzMVA-KCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen, the dataset contains a training, a validation and a test split. And each example consists of an image, a prompt, and a target bounding box."
      ],
      "metadata": {
        "id": "wjI5uyk48V-g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DYk7tDBy-ys"
      },
      "outputs": [],
      "source": [
        "print(dataset['train'].info)\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Let's look at a sample in the dataset\n",
        "import math\n",
        "import json\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# change this index from 0 to split size to see different samples\n",
        "sample = dataset['train'][49]\n",
        "image = sample['image']\n",
        "width, height = image.size\n",
        "print(f\"image width, height: {width, height}\")\n",
        "print(f\"prompt: {sample['prompt']}\")\n",
        "\n",
        "# bb = json.loads(sample[\"target_bounding_box\"])\n",
        "bb = sample[\"target_bounding_box\"]\n",
        "\n",
        "\n",
        "print(f\"target bounding box: {bb}\")\n",
        "\n",
        "xmin = math.floor(width*bb[\"xmin\"])\n",
        "ymin = math.floor(height*bb[\"ymin\"])\n",
        "xmax = math.floor(width*bb[\"xmax\"])\n",
        "ymax = math.floor(height*bb[\"ymax\"])\n",
        "\n",
        "print(f\"to image pixel values: xmin, ymin, xmax, ymax: {xmin, ymin, xmax, ymax}\")\n",
        "\n",
        "shape = [(xmin, ymin), (xmax, ymax)]\n",
        "\n",
        "# create rectangle image\n",
        "img1 = ImageDraw.Draw(image)  \n",
        "img1.rectangle(shape, outline =\"green\", width=5)\n",
        "image.resize((int(width*0.5), int(height*0.5)))\n"
      ],
      "metadata": {
        "id": "6f2fjxGaHWli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCjMK93Cz3zf"
      },
      "source": [
        "## Load model and processor\n",
        "\n",
        "Next, we load the model (which is an instance of [VisionEncoderDecoderModel](https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder), and the processor, which is the object that can be used to prepare inputs for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahkkeo8_o69z"
      },
      "outputs": [],
      "source": [
        "from transformers import VisionEncoderDecoderConfig\n",
        "\n",
        "pretrained_repo_name = REFEXP_MODEL_CHECKPOINT\n",
        "\n",
        "max_length = 128\n",
        "image_size = [1280, 960]\n",
        "\n",
        "# update image_size of the encoder\n",
        "# during pre-training, a larger image size was used\n",
        "config = VisionEncoderDecoderConfig.from_pretrained(pretrained_repo_name)\n",
        "config.encoder.image_size = image_size # (height, width)\n",
        "# update max_length of the decoder (for generation)\n",
        "config.decoder.max_length = max_length\n",
        "# TODO we should actually update max_position_embeddings and interpolate the pre-trained ones:\n",
        "# https://github.com/clovaai/donut/blob/0acc65a85d140852b8d9928565f0f6b2d98dc088/donut/model.py#L602"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84TkZP5zz4hE"
      },
      "outputs": [],
      "source": [
        "from transformers import DonutProcessor, VisionEncoderDecoderModel, BartConfig\n",
        "\n",
        "processor = DonutProcessor.from_pretrained(pretrained_repo_name)\n",
        "model = VisionEncoderDecoderModel.from_pretrained(pretrained_repo_name, config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add special tokens\n",
        "\n",
        "For DocVQA, we add special tokens for \\<yes> and \\<no/>, to make sure that the model (actually the decoder) learns embedding vectors for those explicitly."
      ],
      "metadata": {
        "id": "PfTPbvNRCEDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "def add_tokens(list_of_tokens: List[str]):\n",
        "    \"\"\"\n",
        "    Add tokens to tokenizer and resize the token embeddings\n",
        "    \"\"\"\n",
        "    newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n",
        "    if newly_added_num > 0:\n",
        "        model.decoder.resize_token_embeddings(len(processor.tokenizer))"
      ],
      "metadata": {
        "id": "CfJMb2o31AA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # TODO: Do we need this for UI RefExp? It came from the DocVQA code\n",
        "# additional_tokens = [\"<yes/>\", \"<no/>\"]\n",
        "\n",
        "# add_tokens(additional_tokens)"
      ],
      "metadata": {
        "id": "_dnEFkj71UE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b46s3KR-x8Iv"
      },
      "source": [
        "## Create PyTorch dataset\n",
        "\n",
        "Here we create a regular PyTorch dataset.\n",
        "\n",
        "The model doesn't directly take the (image, JSON) pairs as input and labels. Rather, we create `pixel_values`, `decoder_input_ids` and `labels`. These are all PyTorch tensors. The `pixel_values` are the input images (resized, padded and normalized), the `decoder_input_ids` are the decoder inputs, and the `labels` are the decoder targets.\n",
        "\n",
        "The reason we create the `decoder_input_ids` explicitly here is because otherwise, the model would create them automatically based on the `labels` (by prepending the decoder start token ID, replacing -100 tokens by padding tokens). The reason for that is that we don't want the model to learn to generate the entire prompt, which includes the question. Rather, we only want it to learn to generate the answer. Hence, we'll set the labels of the prompt tokens to -100.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tWX_qJDvw_S"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "from typing import Any, List, Tuple\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "added_tokens = []\n",
        "\n",
        "class DonutDataset(Dataset):\n",
        "    \"\"\"\n",
        "    DonutDataset which is saved in huggingface datasets format. (see details in https://huggingface.co/docs/datasets)\n",
        "    Each row, consists of image blob, prompt and target bounding box.,\n",
        "    and it will be converted into input_tensor(vectorized image) and input_ids(tokenized string).\n",
        "    Args:\n",
        "        dataset_name_or_path: name of dataset (available at huggingface.co/datasets) or the path containing image files and metadata.jsonl\n",
        "        max_length: the max number of tokens for the target sequences\n",
        "        split: whether to load \"train\", \"validation\" or \"test\" split\n",
        "        ignore_id: ignore_index for torch.nn.CrossEntropyLoss\n",
        "        task_start_token: the special token to be fed to the decoder to conduct the target task\n",
        "        prompt_end_token: the special token at the end of the sequences\n",
        "        sort_json_key: whether or not to sort the JSON keys\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_name_or_path: str,\n",
        "        max_length: int,\n",
        "        range_samples: int = None,\n",
        "        shuffle: bool = False,\n",
        "        split: str = \"train\",\n",
        "        ignore_id: int = -100,\n",
        "        task_start_token: str = \"<s>\",\n",
        "        prompt_end_token: str = None,\n",
        "        sort_json_key: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.split = split\n",
        "        self.ignore_id = ignore_id\n",
        "        self.task_start_token = task_start_token\n",
        "        self.prompt_end_token = prompt_end_token if prompt_end_token else task_start_token\n",
        "        self.sort_json_key = sort_json_key\n",
        "\n",
        "        self.dataset = load_dataset(dataset_name_or_path, split=self.split)\n",
        "\n",
        "        self.gt_token_sequences = []\n",
        "        if shuffle:\n",
        "          self.dataset = self.dataset.shuffle()\n",
        "        if range_samples is not None:\n",
        "          self.dataset = self.dataset.select(range_samples)\n",
        "        self.dataset = self.dataset.shuffle()\n",
        "        self.dataset_length = self.dataset.num_rows\n",
        "        for sample in self.dataset:\n",
        "            prompt = sample[\"prompt\"]\n",
        "            # bb = json.loads(sample[\"target_bounding_box\"])\n",
        "            bb = sample[\"target_bounding_box\"]\n",
        "            # Trim float precision to simplify training with shorter string representations of component coordinates.\n",
        "            # 2 decimals precision seems to be a good balance between component position acccuracy and model convergance time.\n",
        "            # 3 decimals precision is good enough for screenshot size up to [1000x1000], but it takes longer for the model to converge.\n",
        "            # For even finer granurality, we cam increase precision to 4 for [10,000 x 10,000] screen sizes, but it will take much more training time and compute resources to converge.\n",
        "            for key, value in bb.items():\n",
        "              bb[key] = round(value,2)\n",
        "\n",
        "            assert isinstance(bb, dict)\n",
        "            ground_truth = {\"prompt\": prompt, \"target_bounding_box\": bb}\n",
        "            gt_json = ground_truth\n",
        "\n",
        "            j2t = self.json2token(\n",
        "                  gt_json,\n",
        "                  update_special_tokens_for_json_key=self.split == \"train\",\n",
        "                  sort_json_key=self.sort_json_key,\n",
        "              ) + processor.tokenizer.eos_token\n",
        "            self.gt_token_sequences.append(j2t)\n",
        "\n",
        "        self.add_tokens([self.task_start_token, self.prompt_end_token])\n",
        "        self.prompt_end_token_id = processor.tokenizer.convert_tokens_to_ids(self.prompt_end_token)\n",
        "\n",
        "    def json2token(self, obj: Any, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n",
        "        \"\"\"\n",
        "        Convert an ordered JSON object into a token sequence\n",
        "        \"\"\"\n",
        "        if type(obj) == dict:\n",
        "            if len(obj) == 1 and \"text_sequence\" in obj:\n",
        "                return obj[\"text_sequence\"]\n",
        "            else:\n",
        "                output = \"\"\n",
        "                if sort_json_key:\n",
        "                    keys = sorted(obj.keys(), reverse=True)\n",
        "                else:\n",
        "                    keys = obj.keys()\n",
        "                for k in keys:\n",
        "                    if update_special_tokens_for_json_key:\n",
        "                        self.add_tokens([fr\"<s_{k}>\", fr\"</s_{k}>\"])\n",
        "                    output += (\n",
        "                        fr\"<s_{k}>\"\n",
        "                        + self.json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
        "                        + fr\"</s_{k}>\"\n",
        "                    )\n",
        "                return output\n",
        "        elif type(obj) == list:\n",
        "            return r\"<sep/>\".join(\n",
        "                [self.json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
        "            )\n",
        "        else:\n",
        "            obj = str(obj)\n",
        "            if f\"<{obj}/>\" in added_tokens:\n",
        "                obj = f\"<{obj}/>\"  # for categorical special tokens\n",
        "            return obj\n",
        "    \n",
        "    def add_tokens(self, list_of_tokens: List[str]):\n",
        "        \"\"\"\n",
        "        Add special tokens to tokenizer and resize the token embeddings of the decoder\n",
        "        \"\"\"\n",
        "        newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n",
        "        if newly_added_num > 0:\n",
        "            model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
        "            added_tokens.extend(list_of_tokens)\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return self.dataset_length - 1\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Load image from image_path of given dataset_path and convert into input_tensor and labels\n",
        "        Convert gt data into input_ids (tokenized string)\n",
        "        Returns:\n",
        "            input_tensor : preprocessed image\n",
        "            input_ids : tokenized gt_data\n",
        "            labels : masked labels (model doesn't need to predict prompt and pad token)\n",
        "        \"\"\"\n",
        "        sample = self.dataset[idx]\n",
        "\n",
        "        # input_tensor\n",
        "        pixel_values = processor(sample[\"image\"].convert(\"RGB\"), random_padding=self.split == \"train\", return_tensors=\"pt\").pixel_values\n",
        "        input_tensor = pixel_values.squeeze()\n",
        "\n",
        "        # input_ids\n",
        "        processed_parse = self.gt_token_sequences[idx]\n",
        "        input_ids = processor.tokenizer(\n",
        "            processed_parse,\n",
        "            add_special_tokens=False,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )[\"input_ids\"].squeeze(0)\n",
        "\n",
        "        if idx % 100 == 0:\n",
        "          print(f\"sameple #{idx}, input_ids: {input_ids}\")\n",
        "\n",
        "        if self.split == \"train\":\n",
        "            labels = input_ids.clone()\n",
        "            labels[\n",
        "                labels == processor.tokenizer.pad_token_id\n",
        "            ] = self.ignore_id  # model doesn't need to predict pad token\n",
        "            labels[\n",
        "                : torch.nonzero(labels == self.prompt_end_token_id).sum() + 1\n",
        "            ] = self.ignore_id  # model doesn't need to predict prompt (for VQA)\n",
        "            return input_tensor, input_ids, labels\n",
        "        else:\n",
        "            prompt_end_index = torch.nonzero(\n",
        "                input_ids == self.prompt_end_token_id\n",
        "            ).sum()  # return prompt end index instead of target output labels\n",
        "            return input_tensor, input_ids, prompt_end_index, processed_parse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_h6nyTm3RN0"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JpazNkf8CnA"
      },
      "outputs": [],
      "source": [
        "# we update some settings which differ from pretraining; namely the size of the images + no rotation required\n",
        "# source: https://github.com/clovaai/donut/blob/master/config/train_cord.yaml\n",
        "processor.feature_extractor.size = image_size[::-1] # should be (width, height)\n",
        "processor.feature_extractor.do_align_long_axis = False\n",
        "\n",
        "# For warm up phase, consider picking only a small subset to see if the model converges on the data\n",
        "max_train_samples = 15000\n",
        "# pick a range for sampling\n",
        "# range_train_samples = range(4000, 4000+max_train_samples)\n",
        "range_train_samples = range(max_train_samples)\n",
        "\n",
        "train_dataset = DonutDataset(REFEXP_DATASET_NAME, max_length=max_length, \n",
        "                             range_samples=range_train_samples,\n",
        "                             shuffle=True,\n",
        "                             split=\"train\", task_start_token=\"<s_refexp>\", prompt_end_token=\"<s_target_bounding_box>\",\n",
        "                             sort_json_key=False,\n",
        "                             )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "vlrKXSzLBAwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# pick a small subset for initial val set to see if validation metrics improve\n",
        "max_val_samples = 800\n",
        "range_val_samples = range(max_val_samples)\n",
        "\n",
        "val_dataset = DonutDataset(REFEXP_DATASET_NAME, max_length=max_length, range_samples=range_val_samples,\n",
        "                             split=\"validation\", task_start_token=\"<s_refexp>\", prompt_end_token=\"<s_target_bounding_box>\",\n",
        "                             sort_json_key=False,\n",
        "                             )\n"
      ],
      "metadata": {
        "id": "cxv4RS8i-rsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_values, decoder_input_ids, labels = train_dataset[0]\n"
      ],
      "metadata": {
        "id": "9ZUvmOdAs7xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_values.shape"
      ],
      "metadata": {
        "id": "AQMuNYnA4XYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels)"
      ],
      "metadata": {
        "id": "vWKlLJML4o-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for decoder_input_id, label in zip(decoder_input_ids.tolist()[:-1], labels.tolist()[1:]):\n",
        "  if label != -100:\n",
        "    print(processor.decode([decoder_input_id]), processor.decode([label]))\n",
        "  else:\n",
        "    print(processor.decode([decoder_input_id]), label)"
      ],
      "metadata": {
        "id": "Zud4yPeN4qQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_values, decoder_input_ids, prompt_end_index, processed_parse = val_dataset[0]\n"
      ],
      "metadata": {
        "id": "0xcQqFDsBmPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_values.shape"
      ],
      "metadata": {
        "id": "Szz2rquaBq89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_end_index"
      ],
      "metadata": {
        "id": "1mUwVF9yBr_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_parse"
      ],
      "metadata": {
        "id": "cj2gybmeBuvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygTIylugfasG"
      },
      "source": [
        "## Create PyTorch DataLoaders\n",
        "\n",
        "Next, we create corresponding PyTorch DataLoaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLQ_Vl5MLugu"
      },
      "outputs": [],
      "source": [
        "print(f\"train dataset length: {train_dataset.dataset_length}\")\n",
        "print(f\"validation dataset length: {val_dataset.dataset_length}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "pIkar2gaX4Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxtTVgNnfdkD"
      },
      "source": [
        "Let's verify a batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHurHlLnL8Xm"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "pixel_values, decoder_input_ids, labels = batch\n",
        "print(pixel_values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input_ids.shape"
      ],
      "metadata": {
        "id": "Vo0TXXDL8oHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see that we have set the labels of all prompt tokens (which includes the prompt) to -100, to make sure the model doesn't learn to generate them. We only start to have labels starting from the \\<s_target_bounding_box> decoder input token."
      ],
      "metadata": {
        "id": "a_GvAiCQkPSf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8ehAwgPZrcc"
      },
      "outputs": [],
      "source": [
        "for decoder_input_id, label in zip(decoder_input_ids[0].tolist()[:-1][:50], labels[0].tolist()[1:][:50]):\n",
        "  if label != -100:\n",
        "    print(processor.decode([decoder_input_id]), processor.decode([label]))\n",
        "  else:\n",
        "    print(processor.decode([decoder_input_id]), label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnmD7rRy2WLI"
      },
      "source": [
        "## Define LightningModule\n",
        "\n",
        "We'll fine-tune the model using [PyTorch Lightning](https://www.pytorchlightning.ai/) here, but note that you can of course also just fine-tune with regular PyTorch, HuggingFace [Accelerate](https://github.com/huggingface/accelerate), the HuggingFace [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer), etc.\n",
        "\n",
        "PyTorch Lightning is pretty convenient to handle things like device placement, mixed precision and logging for you."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Evaluation Metric\n",
        "\n",
        "DocVQA normally uses Edit Distance, but that is not the most natural choice for bounding box evaluation.\n",
        "\n",
        "### Distance between rectangle centers\n",
        "\n",
        "In the early stages of training, center distance is a useful coarse grained eval metric. It tells us how close the center of the predicted bounding box is from the center of the ground truth bounding box.\n",
        "\n",
        "As the model improves, we can switch to a more fine grained eval metric such as IoU. See further below.\n"
      ],
      "metadata": {
        "id": "UXzLACRy1ZLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_center_distance(bb1, bb2):\n",
        "    \"\"\"\n",
        "    Calculate the distance between the centers of two bounding boxes.\n",
        "    Best case, distance between centers of predicted and ground truth bounding boxes will be 0.\n",
        "    Worst case,  distance will be the larges diagonal in the screen - sqrt(1,1).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bb1 : dict\n",
        "        Keys: {'xmin', 'xmax', 'ymin', 'ymax'}\n",
        "        The (xmin, ymin) position is at the top left corner,\n",
        "        the (xmax, y2) position is at the bottom right corner\n",
        "    bb2 : dict\n",
        "        Keys: {'xmin', 'xmax', 'ymin', 'ymax'}\n",
        "        The (x, y) position is at the top left corner,\n",
        "        the (xmax, ymax) position is at the bottom right corner\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        in [0, sqrt(1+1)]\n",
        "    \"\"\"\n",
        "    best_case = 0.0\n",
        "    worst_case = math.sqrt(1+1) # max diagonal\n",
        "    # print(f\"IoU input bb1, bb2: {bb1} , {bb2}\")\n",
        "    # if predictions are not resulting in properly shaped bounding boxes, return no-match\n",
        "    try:\n",
        "      if bb1['xmin'] > bb1['xmax']:\n",
        "        return worst_case # max distance = sqrt(1+1)\n",
        "      if bb1['ymin'] > bb1['ymax']:\n",
        "        return worst_case # max distance\n",
        "      # if any of the bounding box labels are not properly shaped, return no-match\n",
        "      if bb2['xmin'] > bb2['xmax']:\n",
        "        return worst_case  # max distance\n",
        "      if bb2['ymin'] > bb2['ymax']:\n",
        "        return worst_case  # max distance\n",
        "    except Exception as e:\n",
        "      print(f\"Error evaluating center distance between {bb1} and {bb2}\", e)\n",
        "      return worst_case      \n",
        "\n",
        "    # determine the coordinates of the center of each rectangle\n",
        "    bb1_x_center = (bb1['xmax'] + bb1['xmin'])/2\n",
        "    bb1_y_center = (bb1['ymax'] + bb1['ymin'])/2\n",
        "\n",
        "    bb2_x_center = (bb2['xmax'] + bb2['xmin'])/2\n",
        "    bb2_y_center = (bb2['ymax'] + bb2['ymin'])/2\n",
        "    center_dist = math.sqrt((bb2_x_center - bb1_x_center)**2 + (bb2_y_center - bb1_y_center)**2)\n",
        "\n",
        "    assert center_dist >= best_case\n",
        "    assert center_dist <= worst_case # sqrt(1+1)\n",
        "    return center_dist"
      ],
      "metadata": {
        "id": "YYk7Y6SoU9Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Intersection over Union\n",
        "\n",
        "We can use [Intersection over Union](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/) (IoU) to measure bounding box match as a validation progress metric instead of the Edit Distance metric used in DocVQA.\n",
        "\n",
        "Since the model outputs bounding box coordinates, we would like to see these bounding boxes trend towards overlapping exactly with the ground truth.\n",
        "\n",
        "![IoU image](https://i.stack.imgur.com/n1AZj.png)\n",
        "\n",
        "Edit distance also trends towards full match, but it may show less useful intermediate values. For example `xmin=0.123` and `xmin=0.923` are only 1 character separated, but in terms of bounding box overlap, they are very far apart.\n"
      ],
      "metadata": {
        "id": "RKtTK9RWU9kH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_iou(bb1, bb2):\n",
        "    \"\"\"\n",
        "    Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
        "    Best case, IoU is 1 indicating perfect match between prediction and ground truth.\n",
        "    Worst case, IoU is 0 when no overlap between bounding boxes.\n",
        "    Modifed version from the following original on stackoverflow:\n",
        "    https://stackoverflow.com/questions/25349178/calculating-percentage-of-bounding-box-overlap-for-image-detector-evaluation\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bb1 : dict\n",
        "        Keys: {'xmin', 'xmax', 'ymin', 'ymax'}\n",
        "        The (xmin, ymin) position is at the top left corner,\n",
        "        the (xmax, y2) position is at the bottom right corner\n",
        "    bb2 : dict\n",
        "        Keys: {'xmin', 'xmax', 'ymin', 'ymax'}\n",
        "        The (x, y) position is at the top left corner,\n",
        "        the (xmax, ymax) position is at the bottom right corner\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        in [0, 1]\n",
        "    \"\"\"\n",
        "    best_case = 1.0\n",
        "    worst_case = 0.0\n",
        "    # print(f\"IoU input bb1, bb2: {bb1} , {bb2}\")\n",
        "    # if predictions are not resulting in properly shaped bounding boxes, return no-match\n",
        "    try:\n",
        "      if bb1['xmin'] >= bb1['xmax']:\n",
        "        return worst_case\n",
        "      if bb1['ymin'] >= bb1['ymax']:\n",
        "        return worst_case\n",
        "\n",
        "      # if any of the bounding box labels are not properly shaped, return no-match\n",
        "      if bb2['xmin'] >= bb2['xmax']:\n",
        "        return worst_case\n",
        "      if bb2['ymin'] >= bb2['ymax']:\n",
        "        return worst_case\n",
        "    except Exception as e:\n",
        "      print(f\"Error evaluating IoU between {bb1} and {bb2}\", e)\n",
        "      return worst_case\n",
        "\n",
        "    # determine the coordinates of the intersection rectangle\n",
        "    x_left = max(bb1['xmin'], bb2['xmin'])\n",
        "    y_top = max(bb1['ymin'], bb2['ymin'])\n",
        "    x_right = min(bb1['xmax'], bb2['xmax'])\n",
        "    y_bottom = min(bb1['ymax'], bb2['ymax'])\n",
        "\n",
        "    # print(f\"IoU x_left: {x_left}, y_top: {y_top}, x_right: {x_right}, y_bottom: {y_bottom}\")\n",
        "\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return worst_case # no bbox overlap\n",
        "\n",
        "    # The intersection of two axis-aligned bounding boxes is always an\n",
        "    # axis-aligned bounding box\n",
        "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
        "    # print(f\"IoU intersection_area: {intersection_area}\")\n",
        "\n",
        "    # compute the area of both AABBs\n",
        "    bb1_area = (bb1['xmax'] - bb1['xmin']) * (bb1['ymax'] - bb1['ymin'])\n",
        "    bb2_area = (bb2['xmax'] - bb2['xmin']) * (bb2['ymax'] - bb2['ymin'])\n",
        "    # print(f\"IoU bb1_area: {bb1_area}\")\n",
        "    # print(f\"IoU bb2_area: {bb2_area}\")\n",
        "\n",
        "    # compute the intersection over union by taking the intersection\n",
        "    # area and dividing it by the sum of prediction + ground-truth\n",
        "    # areas - the interesection area\n",
        "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
        "    # if iou > 0:\n",
        "    #   print(f\"IoU input bb1, bb2: {bb1} , {bb2}\")\n",
        "    #   print(f\"IoU : {iou}\")\n",
        "    assert iou >= worst_case\n",
        "    assert iou <= best_case\n",
        "    return iou"
      ],
      "metadata": {
        "id": "AmDmvkkfnDef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRm5i4gWG-sb"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "# from nltk import edit_distance\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.utilities import rank_zero_only\n",
        "\n",
        "\n",
        "class DonutModelPLModule(pl.LightningModule):\n",
        "    def __init__(self, config, processor, model, batch_size=8, learning_rate=0.005):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.processor = processor\n",
        "        self.model = model\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        pixel_values, decoder_input_ids, labels = batch\n",
        "        \n",
        "        outputs = self.model(pixel_values,\n",
        "                             decoder_input_ids=decoder_input_ids[:, :-1],\n",
        "                             labels=labels[:, 1:])\n",
        "        loss = outputs.loss\n",
        "        self.log_dict({\"train_loss\": loss}, sync_dist=True)\n",
        "        return loss\n",
        "\n",
        "    def token2bbox(self, seq: str):\n",
        "        target_bbox = self.processor.token2json(seq)\n",
        "        bbox = target_bbox.get('target_bounding_box')\n",
        "        if bbox is None:\n",
        "          print(f\"token2bbox seq has no target_bounding_box, seq:{seq}\")\n",
        "          bbox = bbox = {\"xmin\": 0, \"ymin\": 0, \"xmax\": 0, \"ymax\": 0}\n",
        "          return bbox\n",
        "        # print(f\"token2 bounding box json: {bbox}\")\n",
        "        # safeguard in case text prediction is missing some bounding box coordinates\n",
        "        # or coordinates are not valid numeric values\n",
        "        try:\n",
        "          xmin = float(bbox.get(\"xmin\", 0))\n",
        "        except Exception:\n",
        "          xmin = 0\n",
        "        try:\n",
        "          ymin = float(bbox.get(\"ymin\", 0))\n",
        "        except Exception:\n",
        "          ymin = 0\n",
        "        try:\n",
        "          xmax = float(bbox.get(\"xmax\", 1))\n",
        "        except Exception:\n",
        "          xmax = 1\n",
        "        try:\n",
        "          ymax = float(bbox.get(\"ymax\", 1))\n",
        "        except Exception:\n",
        "          ymax = 1\n",
        "        # replace str with float coords\n",
        "        bbox = {\"xmin\": xmin, \"ymin\": ymin, \"xmax\": xmax, \"ymax\": ymax}\n",
        "        # print(f\"token2 bounding box float: {bbox}\")\n",
        "        return bbox\n",
        "\n",
        "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
        "        pixel_values, decoder_input_ids, prompt_end_idxs, answers = batch\n",
        "        decoder_prompts = pad_sequence(\n",
        "            [input_id[: end_idx + 1] for input_id, end_idx in zip(decoder_input_ids, prompt_end_idxs)],\n",
        "            batch_first=True,\n",
        "        )\n",
        "        \n",
        "        outputs = self.model.generate(pixel_values,\n",
        "                                   decoder_input_ids=decoder_prompts,\n",
        "                                   max_length=max_length,\n",
        "                                   early_stopping=True,\n",
        "                                   pad_token_id=self.processor.tokenizer.pad_token_id,\n",
        "                                   eos_token_id=self.processor.tokenizer.eos_token_id,\n",
        "                                   use_cache=True,\n",
        "                                   num_beams=1,\n",
        "                                   bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n",
        "                                   return_dict_in_generate=True,)\n",
        "    \n",
        "        predictions = []\n",
        "        for seq in self.processor.tokenizer.batch_decode(outputs.sequences):\n",
        "            seq = seq.replace(self.processor.tokenizer.eos_token, \"\").replace(self.processor.tokenizer.pad_token, \"\")\n",
        "            seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
        "            predictions.append(seq)\n",
        "\n",
        "        scores = list()\n",
        "        for pred, answer in zip(predictions, answers):\n",
        "            answer = re.sub(r\"<.*?>\", \"\", answer, count=1)\n",
        "            answer = answer.replace(self.processor.tokenizer.eos_token, \"\")\n",
        "            answer_bbox = self.token2bbox(answer)\n",
        "            pred_bbox = self.token2bbox(pred)\n",
        "            scores.append(get_center_distance(pred_bbox, answer_bbox))\n",
        "            # scores.append(get_iou(pred_bbox, answer_bbox))\n",
        "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
        "              print(f\"      Prediction: {pred}\")\n",
        "              print(f\"          Answer: {answer}\")\n",
        "              print(f\" Prediction bbox: {pred_bbox}\")\n",
        "              print(f\"     Answer bbox: {answer_bbox}\")\n",
        "              print(f\"Eval score (Center Distance): {scores[0]}\")\n",
        "              # print(f\"Eval score (IoU): {scores[0]}\")\n",
        "              # print(f\"Eval score (Edit Distance): {scores[2]}\")\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def validation_epoch_end(self, validation_step_outputs):\n",
        "        # I set this to 1 manually\n",
        "        # (previously set to len(self.config.dataset_name_or_paths))\n",
        "        num_of_loaders = 1\n",
        "        if num_of_loaders == 1:\n",
        "            validation_step_outputs = [validation_step_outputs]\n",
        "        assert len(validation_step_outputs) == num_of_loaders\n",
        "        cnt = [0] * num_of_loaders\n",
        "        total_metric = [0] * num_of_loaders\n",
        "        val_metric = [0] * num_of_loaders\n",
        "        for i, results in enumerate(validation_step_outputs):\n",
        "            for scores in results:\n",
        "                cnt[i] += len(scores)\n",
        "                total_metric[i] += np.sum(scores)\n",
        "            val_metric[i] = total_metric[i] / cnt[i]\n",
        "            val_metric_name = f\"val_metric_{i}th_dataset\"\n",
        "            self.log_dict({val_metric_name: val_metric[i]}, sync_dist=True)\n",
        "        self.log_dict({\"val_metric\": np.sum(total_metric) / np.sum(cnt)}, sync_dist=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # TODO add scheduler\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.get(\"lr\"))\n",
        "    \n",
        "        return optimizer\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return train_dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return val_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we instantiate the module:"
      ],
      "metadata": {
        "id": "bKujfvIDlAHo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxNJhCGjKhtR"
      },
      "outputs": [],
      "source": [
        "config = {\"max_epochs\": 2, # aim for 30,\n",
        "          \"val_check_interval\":0.4, # how many times we want to validate during an epoch\n",
        "          \"check_val_every_n_epoch\":1,\n",
        "          \"gradient_clip_val\":1.0,\n",
        "          \"num_training_samples_per_epoch\": 800,\n",
        "          \"lr\":3e-6, # Start at 3e-5 and reduce gradually every few epochs if loss oscilations too high\n",
        "          \"train_batch_sizes\": [8],\n",
        "          \"val_batch_sizes\": [1],\n",
        "          # \"seed\":2022,\n",
        "          \"num_nodes\": 1,\n",
        "          \"warmup_steps\": 20, # 20 = 800/8*2/10, 10%; 300 for 800/8*30/10, 10%\n",
        "          \"result_path\": \"./result\",\n",
        "          \"verbose\": True,\n",
        "          }\n",
        " \n",
        "model_module = DonutModelPLModule(config, processor, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZoPiDOPKg0o"
      },
      "source": [
        "## Train!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "# clear any previously open logging session\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "GQhXL0AdWpuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiK6-vQHKnBy"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.loggers import WandbLogger\n",
        " \n",
        "\n",
        "wandb_logger = WandbLogger(project=\"Donut-RefExp\")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "        accelerator=\"auto\",\n",
        "        devices=\"auto\",\n",
        "        auto_scale_batch_size=True,\n",
        "        auto_lr_find=True,\n",
        "        max_epochs=config.get(\"max_epochs\"),\n",
        "        val_check_interval=config.get(\"val_check_interval\"),\n",
        "        check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
        "        gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
        "        precision=16, # we'll use mixed precision\n",
        "        num_sanity_val_steps=0,\n",
        "        logger=wandb_logger,\n",
        "        # callbacks=[lr_callback, checkpoint_callback],\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ Tune the model to discover optimal hyper-parameters\n",
        "trainer.tune(model_module)\n",
        "print(f\"Recommended batch_size: {model_module.batch_size}\")\n",
        "\n",
        "# If recommended batch-size is different from the value used to initialize DataLoaders above, \n",
        "# consider re-initializing data loaders with new recommended value.\n",
        "\n",
        "print(f\"Recommended learning rate: {model_module.learning_rate}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Iax7Rz8sJnIx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}